<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Visual Med-Alpaca</title>
<link rel="shortcut icon" href="favicon.ico">
<link rel="stylesheet" href="files/style.css">
<link rel="stylesheet" href="files/font.css">

</head>

<style type="text/css">
	#myvalignContainer1O { position:relative }
	#myvalignContainer1I { position:absolute; top:50%; height:10em; margin-top:-5em }
</style>
<style type="text/css">
	#myvalignContainer2 { line-height:4em }
</style>

<body>

<script src="files/analytics.js" async=""></script><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-40457306-2', 'mit.edu');
  ga('send', 'pageview');

</script>

<!-- Title -->
<div class="container">

<!-- <span class="title">Task-Oriented Flow Utilization</span> -->
<!-- <span class="venue">Conference name</span> -->
    <td><center><img src="files/ltl_logo.png" width="1000" ></center></td>
    <br><br>
<span class="title">Visual Med-Alpaca: A Parameter-Efficient Biomedical LLM with Visual Capabilities</span>

<table align="center" border="0" width="1000" class="authors">
	<tbody><tr>
	<td class="author"> <a href="https://ciaranshu.github.io">Chang Shu</a><sup>1*</sup></td>
	<td class="author"> <a href="https://scholar.google.com/citations?user=IFKToXUAAAAJ&hl=en&oi=ao">Baian Chen</a><sup>2*</sup></td>
	<td class="author"> <a href="http://fangyuliu.me">Fangyu Liu</a><sup>1</sup></td>
	<td class="author"> <a href="https://fuzihaofzh.github.io">Zihao Fu</a><sup>1</sup></td>
    <td class="author"> <a href="https://eehsan.github.io">Ehsan Shareghi </a><sup>3</sup></td>
	<td class="author"> <a href="https://sites.google.com/site/nhcollier/home/">Nigel Collier</a><sup>1</sup></td>
	</tr></tbody>
</table>

<table align="center" border="0" width="1000" class="affiliations">
<tbody>
	<tr>
    <td class="affliation" align="center">
      <sup>1</sup><a href="https://ltl.mmll.cam.ac.uk">University of Cambridge</a>
      &emsp;&emsp;&emsp;&emsp;
      <sup>2</sup>Ruiping Health</a>
    &emsp;&emsp;&emsp;&emsp;
    <sup>3</sup><a href="https://www.monash.edu/it/dsai">Monash University</a>
    </td>
  </tr>
</tbody>
</table>



<br>
<table align="center"><tbody><tr>
</tr>
<tr><td>
<table border="0">
</tbody>
<tr><td class="caption"> Introducing <a href="https://github.com/cambridgeltl/visual-med-alpaca"><strong>Visual Med-Alpaca</strong></a>, an open-source, parameter-efficient biomedical foundation model that can be integrated with medical &quot;visual experts&quot; for multimodal biomedical tasks. Built upon the <a href="https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/">LLaMa-7B</a> architecture (<a href="https://arxiv.org/abs/2302.13971">Touvron et al., 2023</a>), this model is trained using an instruction set curated collaboratively by GPT-3.5-Turbo and human experts. Leveraging a few hours of instruction-tuning and the inclusion of plug-and-play visual modules, Visual Med-Alpaca can perform a diverse range of tasks, from interpreting radiological images to addressing complex clinical inquiries. The model can be replicated with ease, necessitating only a single consumer GPU. </td></tr>
<tr><td class="caption"><br> Refer to our <a href="https://github.com/cambridgeltl/visual-med-alpaca"><strong>Official Github Repo</strong></a> for code and data.</td></tr>
</tbody></table>
<br>

<!-- Result -->
<div class="section">
<span class="section-title"> Demo</span>
</br></br>
<table align="center"><tbody>
<tr><td><center>
<img src="files/demo.gif" width="900" >
</td></tr>
</table>
</div>

<!-- Abstract -->
<div class="section">
<span class="section-title">Overview </span>
<p>Domain-specific foundation models play a critical role in the biomedical field, as the language used in biomedical texts is highly specialized, often encompassing domain-specific concepts and relationships not found in general domain text corpora such as Wikipedia and Books. Empirical evidence demonstrates that pretraining on substantial amounts of biomedical text significantly improves language models&#39; performance on various biomedical text mining tasks, as compared to existing publicly available pretrained language models (PLMs) (<a href="https://arxiv.org/abs/1901.08746">Lee et al., 2019</a>; <a href="https://arxiv.org/abs/2004.10964">Gururangan et al., 2020</a>, <a href="https://arxiv.org/pdf/2007.15779.pdf">Gu et al., 2021</a>).</p>
<p>Modern large language models (LLMs) necessitate an unprecedented level of computational resources for full-model fine-tuning. The cost of fine-tuning even a 7-billion-parameter LLM exclusively on PubMed is prohibitively expensive for the majority of academic institutions. Pretraining models on extensive medical image datasets to attain multimodal capabilities incurs even higher costs. Consequently, researchers are exploring more cost-effective techniques such as Adapter, Instruct-Tuning, and Prompt Augmentation to develop models that can be trained and deployed on consumer-level graphics cards while maintaining adequate performance. In the context of bridging text and vision for multimodal applications, training can also be similarly expensive (<a href="https://arxiv.org/abs/2204.14198">Alayrac et al., 2022</a>). Besides, to the best of our knowledge, there is no publicly available multimodal generative foundation model specifically designed for biomedical applications. </p>
<p>In response to these challenges, we introduce <a href="https://github.com/cambridgeltl/visual-med-alpaca"><strong>Visual Med-Alpaca</strong></a>, an open-source, parameter-efficient biomedical foundation model that features a plug-and-play visual extension framework. To develop the Visual Med-Alpaca model, we initially create a biomedical instruction set by extracting medical questions from various medical datasets within the <a href="https://github.com/bigscience-workshop/biomedical">BigBIO</a> repository (<a href="https://arxiv.org/abs/2206.15076">Fries et al., 2022</a>). Subsequently, we prompt GPT-3.5-Turbo to synthesize answers for these questions. Multiple rounds of human filtering and editing are performed to refine the question-answer pairs, resulting in a high-quality instruction set comprising 54k data points. Next, we expand Med-Alpaca into Visual Med-Alpaca by connecting the textual model with &quot;visual medical experts,&quot; which are specialized medical computer vision models. For instance, in radiology-domain applications, we train an in-house radiology image captioning model called Med-GIT (see later for details). When given an input image, a classifier determines if or which medical visual expert is responsible for the image. The designated medical expert then converts the image into a text prompt. The prompt manager subsequently merges the converted visual information with the textual query, enabling Med-Alpaca to generate an appropriate response.</p>
<p><strong>Ongoing work.</strong> A paramount objective for the future is to thoroughly assess the medical proficiency and potential shortcomings of Visual Med-Alpaca, encompassing issues such as misleading medical advice and incorrect medical information. Moving beyond traditional benchmarking and manual evaluation methods, we aim to focus on different user groups, including doctors and patients, and evaluate all facets of the model through a user-centered approach. This comprehensive assessment will enable us to ensure the reliability and effectiveness of Visual Med-Alpaca in addressing various biomedical tasks and catering to the diverse needs of its users.</p>
<p><strong>It is also important to note that Visual Med-Alpaca is strictly intended for academic research purposes and not legally approved for medical use in any country.</strong></p>

<!-- <p class="bibtex">
@art<p>Domain-specific foundation models play a critical role in the biomedical field, as the language used in biomedical texts is highly specialized, often encompassing domain-specific concepts and relationships not found in general domain text corpora such as Wikipedia and Books. Empirical evidence demonstrates that pretraining on substantial amounts of biomedical text significantly improves language models&#39; performance on various biomedical text mining tasks, as compared to existing publicly available pretrained language models (PLMs) (<a href="https://arxiv.org/abs/1901.08746">Lee et al., 2019</a>; <a href="https://arxiv.org/abs/2004.10964">Gururangan et al., 2020</a>, <a href="https://arxiv.org/pdf/2007.15779.pdf">Gu et al., 2021</a>).</p>
<p>Modern large language models (LLMs) necessitate an unprecedented level of computational resources for full-model fine-tuning. The cost of fine-tuning even a 7-billion-parameter LLM exclusively on PubMed is prohibitively expensive for the majority of academic institutions. Pretraining models on extensive medical image datasets to attain multimodal capabilities incurs even higher costs. Consequently, researchers are exploring more cost-effective techniques such as Adapter, Instruct-Tuning, and Prompt Augmentation to develop models that can be trained and deployed on consumer-level graphics cards while maintaining adequate performance. In the context of bridging text and vision for multimodal applications, training can also be similarly expensive (<a href="https://arxiv.org/abs/2204.14198">Alayrac et al., 2022</a>). Besides, to the best of our knowledge, there is no publicly available multimodal generative foundation model specifically designed for biomedical applications. </p>
<p>In response to these challenges, we introduce <a href="https://github.com/cambridgeltl/visual-med-alpaca"><strong>Visual Med-Alpaca</strong></a>, an open-source, parameter-efficient biomedical foundation model that features a plug-and-play visual extension framework. To develop the Visual Med-Alpaca model, we initially create a biomedical instruction set by extracting medical questions from various medical datasets within the <a href="https://github.com/bigscience-workshop/biomedical">BigBIO</a> repository (<a href="https://arxiv.org/abs/2206.15076">Fries et al., 2022</a>). Subsequently, we prompt GPT-3.5-Turbo to synthesize answers for these questions. Multiple rounds of human filtering and editing are performed to refine the question-answer pairs, resulting in a high-quality instruction set comprising 54k data points. Next, we expand Med-Alpaca into Visual Med-Alpaca by connecting the textual model with &quot;visual medical experts,&quot; which are specialized medical computer vision models. For instance, in radiology-domain applications, we train an in-house radiology image captioning model called Med-GIT (see later for details). When given an input image, a classifier determines if or which medical visual expert is responsible for the image. The designated medical expert then converts the image into a text prompt. The prompt manager subsequently merges the converted visual information with the textual query, enabling Med-Alpaca to generate an appropriate response.</p>
<p><strong>Ongoing work.</strong> A paramount objective for the future is to thoroughly assess the medical proficiency and potential shortcomings of Visual Med-Alpaca, encompassing issues such as misleading medical advice and incorrect medical information. Moving beyond traditional benchmarking and manual evaluation methods, we aim to focus on different user groups, including doctors and patients, and evaluate all facets of the model through a user-centered approach. This comprehensive assessment will enable us to ensure the reliability and effectiveness of Visual Med-Alpaca in addressing various biomedical tasks and catering to the diverse needs of its users.</p>
<p><strong>It is also important to note that Visual Med-Alpaca is strictly intended for academic research purposes and not legally approved for medical use in any country.</strong></p>
icle{xue2019video,
  title={Video Enhancement with Task-Oriented Flow},
  author={Xue, Tianfan and Chen, Baian and Wu, Jiajun and Wei, Donglai and Freeman, William T},
  journal={International Journal of Computer Vision (IJCV)},
  volume={127},
  number={8},
  pages={1106--1125},
  year={2019},
  publisher={Springer}
}
</p> -->

</div>


<div class="section">
<span class="section-title"> Model Architecture and Training Pipeline </span>
</br></br>

<center><img src="files/model.png" width="1000" ></center></br>
Visual Med-Alpaca bridges the textual and visual modalities through the prompt augmentation method. Firstly, the image input is fed into a type classifier to identify the appropriate module for converting visual information into an intermediate text format, which is then appended to the text inputs for subsequent reasoning procedures. For instance, medical plots are transformed into intermediate linearized tables through the use of the <a href="https://huggingface.co/docs/transformers/main/model_doc/deplot">DePlot</a> module. The prompt manager then merges the textual information extracted from images and text inputs into the prompt for Med-Alpaca, a large language model used for generating responses with the expertise in biomedical domain.
</br></br>

To incorporate biomedical knowledge and visual modality into the foundation model LLaMA-7B, we carried out fine-tuning using two distinct datasets. Initially, we performed standard fine-tuning and low-rank adaptation (LoRA) fine-tuning on LLaMA-7B model using a model-generated dataset comprising of 54,000 biomedical examples for instruction-tuning purposes. Secondly, we fine-tuned the <a href="https://github.com/microsoft/GenerativeImage2Text">Microsoft GIT</a> model on the <a href="https://github.com/razorx89/roco-dataset">Radiology Objects in Context (ROCO)</a>  dataset to incorporate visual modality.</br></br>

</div>

<div class="section">
<span class="section-title"> Domain Adaptation: Self-Instruct in Biomedical Domain</span>
<p>The process of collecting inquiries from various medical question-and-answer datasets (<a href="https://huggingface.co/datasets/bigbio/mediqa_rqe">MEDIQA RQE</a>, <a href="https://huggingface.co/datasets/bigbio/med_qa">MedQA</a>, <a href="https://huggingface.co/datasets/bigbio/meddialog">MedDialog</a>, <a href="https://huggingface.co/datasets/bigbio/mediqa_qa">MEDIQA QA</a>, <a href="https://huggingface.co/datasets/bigbio/pubmed_qa">PubMedQA</a>) is implemented in our study. This approach aims to increase the diversity and thoroughness of the dataset and improve the accuracy and comprehensiveness of the obtained results.  </p>
<p>We synthesize answers of these questions with GPT-3.5-Turbo in the <a href="https://github.com/yizhongw/self-instruct">self-instruct</a> fashion. The GPT-3.5-Turbo model is equipped with advanced natural language processing capabilities that enable it to understand and generate human-like responses to a wide range of questions. This makes it a reliable tool for generating structural and informative answers.  </p>
<p>The process of filtering and editing question-answer pairs was performed manually. A total of 54,000 turns were carefully selected, taking into account the criteria of balance and diversity.  </p>

</div>


<div class="section">
<span class="section-title"> Visual Adaptation: Radiology Image Captioning, Deplot, and More </span>

<p>Visual input constitutes a vital component of the medical domain, supplying indispensable information in healthcare environments. Healthcare professionals extensively depend on visual cues for diagnosis, monitoring, and treatment of patients. Medical imaging technologies, such as X-rays, CT scans, and MRIs, offer unparalleled insight into internal organs, detecting diseases and abnormalities that may be invisible to the naked eye. Additionally, scientific figures and medical records, including plots, charts, and tables, are prevalent in the medical field.</p>
<p>We propose linking visual experts with Med-Alpaca, as foundation model chaining presents a modular and highly adaptable framework for incorporating a diverse array of visual modules. Within this framework, any multimodal task can be divided into two essential stages: (1) the conversion of images to text, and (2) cognitive reasoning based on the derived text. In our context, visual experts (i.e., visual foundation models) transform medical images into an intermediate text representation. This converted data is then used to prompt a pretrained LLM, leveraging the inherent few-shot reasoning capabilities of LLMs to generate appropriate responses.</p>
<p>Currently, our platform supports two distinct visual experts: Med-GIT and <a href="https://huggingface.co/docs/transformers/main/model_doc/deplot">DePlot</a>, chosen due to the widespread presence of radiology images and plots within the medical domain. The system&#39;s architecture is also designed to enable seamless integration of alternative medical visual experts, and we plan to incorporate additional medical visual foundation models as visual experts in the near future.</p>
<p>The Med-GIT model represents a <a href="https://github.com/microsoft/GenerativeImage2Text">GIT</a>: Generative Image-to-text Transformer for Vision and Language, fine-tuned specifically on the <a href="https://github.com/razorx89/roco-dataset">ROCO</a> dataset to facilitate specialized radiology image captioning. The training procedure for the model is outlined in comprehensive detail in our <a href="https://github.com/cambridgeltl/visual-med-alpaca/tree/main/code">publicly accessible Github repository</a>.  </p>


</div>

<!-- div class="section">
<span class="section-title">Results </span></br>
<p class="subsection">Interpolation</p -->
<!-- Result start -->
<!-- Result end -->

<!-- p class="subsection">Visualization</p -->
<!-- Visualization start -->
<!-- Visualization end -->
<!-- /div -->

<div class="section">
<span class="section-title"> Case Study </span>
<p><strong>Input 1: What are the chemicals that treat hair loss?</strong></p>
<ul>
<li><strong>Visual Med-Alpaca</strong>: There are several chemicals that can be used to treat hair loss, including: 1. Minoxidil: a medication that is applied to the scalp to promote hall growth. 2. Finasteride: a medication that is taken orally to slow down the production of dihydrotestosterone (DHT), a hormone that can cause hair loss. 3. Corticosteroids: topical medications that can be applied to the scalp to reduce inflammation and promote hair growth. 4. Anthralin: a medication that is applied to the scalp to promote hair growth by stimulating the production of blood cells that nourish the hair follicles. 5, Ketoconazole: a medication that is applied to the scalp to treat fungal infections that can cause hair loss. It is important to consult with a healthcare professional before using any of these chemicals to treat hair loss, as they can have potential side effects and may not be suitable for everyone.</li>
</ul>
<details><summary>OTHER METHODS</summary>

<ul>
<li><a href="https://huggingface.co/spaces/tloen/alpaca-lora"><strong>Alpaca-LoRA</strong></a>: The chemicals that treat hair loss include minoxidil, finasteride, and ketoconazole.</li>
<li><a href="https://huggingface.co/spaces/csuhan/LLaMA-Adapter"><strong>LLama-Adapter</strong></a>: Chemicals that treat hair loss include minoxidil, finasteride, and ketoconazole. Minoxidil is a topical solution that is applied directly to the scalp and is used to stimulate hair growth. Finasteride is a pill that is taken orally and is used to block the production of the hormone dihydrotestosterone, which can cause hair loss. Ketoconazole is an antifungal medication that is used to treat scalp infections that can cause hair loss.</li>
<li><a href="https://chat.openai.com/chat"><strong>ChatGPT (GPT-3.5-Turbo)</strong></a>: There are several chemicals that are used to treat hair loss, including: 1. Minoxidil: This is an over-the-counter medication that is applied topically to the scalp. It works by increasing blood flow to the hair follicles, which can promote hair growth. 2. Finasteride: This is a prescription medication that is taken orally. It works by blocking the conversion of testosterone to dihydrotestosterone (DHT), which is a hormone that can cause hair loss. 3. Ketoconazole: This is an antifungal medication that is sometimes used to treat hair loss. It works by reducing inflammation and blocking the production of DHT. 4. Corticosteroids: These are anti-inflammatory medications that can be injected into the scalp. They are sometimes used to treat hair loss caused by autoimmune disorders or inflammation. 5. Spironolactone: This is a medication that is often used to treat high blood pressure, but it can also be effective in treating hair loss in women. It works by blocking the production of androgens, which can contribute to hair loss. It&#39;s important to note that the effectiveness of these treatments can vary depending on the cause and severity of hair loss. It&#39;s best to consult with a healthcare provider or a dermatologist to determine the most appropriate treatment for you.</li>
</ul>
 </details>


<p><strong>Input 2: What is seen in the X-ray and what should be done?</strong><br><strong>Image:</strong><br><center><img src="files/radio.jpg" alt=""></center></p>
<ul>
<li><strong>Visual Med-Alpaca</strong>: Bilateral infiltrates and bilateral pleural effusion seen in a chest x-ray may indicate a respiratory infection or other respiratory conditions such as pneumonia pulmonary edema, or pleural effusion, it is important to consult a medical professional for a proper diagnosis and treatment plan.</li>
</ul>
<details><summary>OTHER METHODS</summary>

<ul>
<li><a href="https://huggingface.co/spaces/BigData-KSU/VQA-in-Medical-Imagery"><strong>VQA Medical Model</strong></a>: the decubitus</li>
<li><a href="https://huggingface.co/spaces/StanfordAIMI/radiology_report_generation"><strong>Automatic Radiology Report Generation (Image-only)</strong></a>: near complete opacification of the left hemi thorax with only a small amount of residual aerated lung in the left upper lobe .</li>
<li><a href="https://chat.openai.com/chat"><strong>ChatGPT (GPT-3.5-Turbo)</strong></a> + <a href="https://huggingface.co/spaces/StanfordAIMI/radiology_report_generation"><strong>Automatic Radiology Report Generation (Image-only)</strong></a>: Opacification of the lung on an X-ray indicates that there is an abnormality in the lung tissue. In this case, the near-complete opacification of the left hemithorax with only a small amount of residual aerated lung in the left upper lobe suggests that there is likely a significant pathology present in the left lung. Possible causes of this finding include pneumonia, lung abscess, tumor or mass, or pleural effusion (fluid accumulation around the lung). It is important to determine the underlying cause of this finding as soon as possible. A follow-up diagnostic workup may include a CT scan of the chest, sputum culture, bronchoscopy, and/or a biopsy of any suspicious areas in the lung. The appropriate treatment will depend on the underlying cause, but it may include antibiotics, drainage of any fluid collections, or surgery to remove any masses or tumors. Prompt evaluation and treatment are necessary to prevent complications and improve outcomes.</li>
</ul>
</details>


<p><strong>Input 3: How effective is this treatment on papule?</strong><br><strong>Image:</strong><br><center><img src="files/bar.png"  width="50%"></center></p>
<ul>
<li><strong>Visual Med-Alpaca</strong>: Based on the information provided, it appears that the treatment has been effective in reducing the size of the papule, The before and after images show a significant reduction in the size of the papule, indicating that the treatment has been successful, However, it is important to note that the control group may not have received any treatment, which could have also contributed to the reduction in size. It would be best to consult with a medical professional for a more accurate assessment of the effectiveness of the treatment.</li>
</ul>
<details><summary>OTHER METHODS</summary>

<ul>
<li><a href="https://huggingface.co/spaces/fl399/deplot_plus_llm"><strong>DePlot + GPT-3.5-Turbo</strong></a>: The table does not provide enough information to answer this question.</li>
<li><a href="https://huggingface.co/spaces/fl399/matcha_chartqa"><strong>MatCha</strong></a>: 150</li>
<li><a href="https://chat.openai.com/chat"><strong>ChatGPT (GPT-3.5-Turbo, Text-only)</strong></a>: I&#39;m sorry, but I need more information to provide an accurate answer. There are many different types of papules, which are small, raised bumps on the skin, and each type may require different treatments. Additionally, the effectiveness of any particular treatment can depend on various factors, such as the severity of the condition, the individual&#39;s overall health, and the specifics of the treatment itself. If you can provide more information about the specific type of papule you are referring to and the treatment in question, I may be able to provide a more helpful answer.</li>
</ul>
</details>


</div>


<div class="section">
<span class="section-title"> Future Work </span>
<p>One of the most crucial ongoing works is the systematic evaluation of Visual Med-Alpaca, as well as other NLP models within the biomedical field. With the varying structure and type of medical data, it is essential to assess the efficacy of NLP models and their generalizability across different datasets.  </p>
<p>We also expect pretraining on medical data can enhance the performance of NLP models in the biomedical field. It should help in the identification and reasoning of disease phenotypes, drug mechanism and the representation of clinical concepts.  </p>
<p>The addition of genome protein modality may also help in achieving better reasoning in LLMs. Given that genetic and protein information are critical for understanding disease processes, LLMs can aid in the analysis of large volumes of genomic data, making it possible to identify novel mutations involved in various disease processes. Therefore, incorporating genomic information into LLMs will enable a wider range of applications within the biomedical field.</p>


</div>


<div class="section">
<span class="section-title"> Implementation Details  </span>
</br></br>
We follow the hyper-parameters as reported in the Github repo of <a href="https://github.com/tloen/alpaca-lora">Alpaca-LoRA</a> and <a href="https://github.com/tatsu-lab/stanford_alpaca">Alpaca</a>:
<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}
</style>
<table class="tg">
<thead>
  <tr>
    <th class="tg-0pky">Model</th>
    <th class="tg-0pky"><span style="font-weight:400;font-style:normal;text-decoration:none">Batch size</span></th>
    <th class="tg-0pky"><span style="font-weight:400;font-style:normal;text-decoration:none">Learning rate</span></th>
    <th class="tg-0pky"><span style="font-weight:400;font-style:normal;text-decoration:none">Epochs</span></th>
    <th class="tg-0pky"><span style="font-weight:400;font-style:normal;text-decoration:none">Max length</span></th>
    <th class="tg-0pky"><span style="font-weight:400;font-style:normal;text-decoration:none">Weight decay</span></th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-0pky">Med-Alpaca-7B</td>
    <td class="tg-0pky"><span style="font-weight:400;font-style:normal;text-decoration:none">128</span></td>
    <td class="tg-0pky">2e-5</td>
    <td class="tg-0pky"><span style="font-weight:400;font-style:normal;text-decoration:none">3</span></td>
    <td class="tg-0pky"><span style="font-weight:400;font-style:normal;text-decoration:none">512</span></td>
    <td class="tg-0pky"><span style="font-weight:400;font-style:normal;text-decoration:none">0</span></td>
  </tr>
  <tr>
    <td class="tg-0pky"><span style="font-weight:400;font-style:normal;text-decoration:none">Med-Alpaca</span>-7B-LoRA</td>
    <td class="tg-0pky"><span style="font-weight:400;font-style:normal;text-decoration:none">128</span></td>
    <td class="tg-0pky"><span style="font-weight:400;font-style:normal;text-decoration:none">1e-4</span></td>
    <td class="tg-0pky"><span style="font-weight:400;font-style:normal;text-decoration:none">3</span></td>
    <td class="tg-0pky"><span style="font-weight:400;font-style:normal;text-decoration:none">512</span></td>
    <td class="tg-0pky">-</td>
  </tr>
</tbody>
</table>
</br>
Hardware and Training Time:
<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}
</style>
<table class="tg">
<thead>
  <tr>
    <th class="tg-0pky">Model</th>
    <th class="tg-0pky"><span style="font-weight:400;font-style:normal;text-decoration:none">CPU count</span></th>
    <th class="tg-0pky"><span style="font-weight:400;font-style:normal;text-decoration:none">GPU count</span></th>
    <th class="tg-0pky"><span style="font-weight:400;font-style:normal;text-decoration:none">GPU type</span></th>
    <th class="tg-0pky"><span style="font-weight:400;font-style:normal;text-decoration:none">Train time </span></th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-0pky">Med-Alpaca-7B</td>
    <td class="tg-0pky"><span style="font-weight:400;font-style:normal;text-decoration:none">128</span></td>
    <td class="tg-0pky">4</td>
    <td class="tg-0pky"><span style="font-weight:400;font-style:normal;text-decoration:none">NVIDIA A100-SXM4-80GB</span></td>
    <td class="tg-0pky">2.51 hours</td>
  </tr>
  <tr>
    <td class="tg-0pky"><span style="font-weight:400;font-style:normal;text-decoration:none">Med-Alpaca</span>-7B-LoRA</td>
    <td class="tg-0pky"><span style="font-weight:400;font-style:normal;text-decoration:none">8</span></td>
    <td class="tg-0pky"><span style="font-weight:400;font-style:normal;text-decoration:none">1</span></td>
    <td class="tg-0pky"><span style="font-weight:400;font-style:normal;text-decoration:none">NVIDIA GeForce RTX 3090 Ti</span></td>
    <td class="tg-0pky">6.55 hours</td>
  </tr></br></br>
</tbody>
</table>
</div>


<div class="section">
<span class="section-title"> Disclaimers </span>
<p>Visual Med-Alpaca, is intended for academic research purposes only. Any commercial or clinical use of the model is strictly prohibited. This decision is based on the <a href="https://docs.google.com/forms/d/e/1FAIpQLSfqNECQnMkycAp2jP4Z9TFX0cGR4uf7b_fBxjY_OjhJILlKGA/viewform">License Agreement</a> inherited from LLaMA, on which the model is built. Additionally, Visual Med-Alpaca is not legally approved for medical use in any country. Users should be aware of the model&#39;s limitations in terms of medical knowledge and the possibility of misinformation. Therefore, any reliance on Visual Med-Alpaca for medical decision-making is at the user&#39;s own risk.  </p>
<p><strong>Note: The developers and owners of the model, the Language Technology Lab at Cambridge University, do not assume any liability for the accuracy or completeness of the information provided by Visual Med-Alpaca, nor will they be responsible for any potential harm caused by the misuse of the model.</strong> </p>


</div>

<div class="section">
<span class="section-title"> Acknowledgement </span>
</br></br>
We are deeply grateful for the contributions made by open-source projects: 
<a href="https://github.com/facebookresearch/llama">LLaMA</a>, 
<a href="https://github.com/tatsu-lab/stanford_alpaca">Stanford Alpaca</a>, 
<a href="https://github.com/tloen/alpaca-lora">Alpaca-LoRA</a>, 
<a href="https://huggingface.co/docs/transformers/main/model_doc/deplot">Deplot</a>, 
<a href="https://huggingface.co/bigbio">BigBio</a>,
<a href="https://github.com/razorx89/roco-dataset">ROCO</a>,
<a href="https://github.com/microsoft/visual-chatgpt">Visual-ChatGPT</a>,
<a href="https://github.com/microsoft/GenerativeImage2Text">GenerativeImage2Text</a>.
</br></br>
</div>
<p>&nbsp;</p>
<!-- end .container --></div>



</body></html>
